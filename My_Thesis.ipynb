{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MdSourav76046/AI_Lab-4.1/blob/main/My_Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Xqc9YhYLK6"
      },
      "source": [
        "## **This code will connect my model with the google vision OCR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUTTU-osKUfM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Replace 'source.json' with your actual JSON file name\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/google_ocr/source.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6UnH6OoPgm8",
        "outputId": "6cc070f5-d293-4e21-91a1-8b09ba81e1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (3.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (4.25.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2024.12.14)\n",
            "Google Vision API authenticated successfully!\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-vision\n",
        "from google.cloud import vision\n",
        "\n",
        "# Initialize the Vision API client\n",
        "try:\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "    print(\"Google Vision API authenticated successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5po-wqOHaIuZ"
      },
      "source": [
        "## **This code will extract the text from the image using google vision OCR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "MmJ5tdtKQIGn",
        "outputId": "ce33ee20-2ba8-4b27-94b5-cc08705dc4c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-48220f4d-aee4-4b12-81c9-e57361215695\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-48220f4d-aee4-4b12-81c9-e57361215695\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving IMG_1739.jpeg to IMG_1739 (2).jpeg\n",
            "Extracted Text:\n",
            "‡¶¶‡ßÅ‡¶á ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶¢‡¶æ‡¶ï‡¶æ ‡¶¶‡¶ø‡¶§‡¶õ‡¶ø \" \" 1 ‡¶ú‡¶ø‡¶§‡¶õ‡¶ø ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶¶‡ßÅ‡¶á ‡¶ú‡¶ø‡¶§‡¶õ‡¶ø ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶¶‡ßÅ‡¶á ‡¶ú‡¶ø‡¶§‡¶õ‡¶ø ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶¶‡ßÅ‡¶á\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import io\n",
        "import os\n",
        "# Import the necessary library\n",
        "from google.cloud import vision\n",
        "\n",
        "\n",
        "# Set up authentication using the uploaded JSON key file\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/google_ocr/source.json\"\n",
        "\n",
        "# Initialize the Vision API client\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "# Function to extract text from an image\n",
        "def extract_text_from_image(image_path):\n",
        "    with io.open(image_path, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    response = client.text_detection(image=image)\n",
        "    texts = response.text_annotations\n",
        "\n",
        "    if not texts:\n",
        "        return \"No text detected in the image.\"\n",
        "\n",
        "    # Extract bounding boxes and text\n",
        "    text_data = []\n",
        "    for text in texts[1:]:  # Skip the first entry (it includes all the text)\n",
        "        bounding_box = text.bounding_poly.vertices\n",
        "        top_left_y = bounding_box[0].y\n",
        "        top_left_x = bounding_box[0].x\n",
        "        text_data.append((top_left_y, top_left_x, text.description))\n",
        "\n",
        "    # Sort the text data by y-coordinate, then x-coordinate\n",
        "    text_data.sort(key=lambda x: (x[0], x[1]))\n",
        "\n",
        "    # Combine the sorted text\n",
        "    sorted_text = \" \".join([item[2] for item in text_data])\n",
        "    return sorted_text\n",
        "\n",
        "# Upload the image to be processed\n",
        "uploaded_image = files.upload()\n",
        "\n",
        "# Get the uploaded image file name\n",
        "image_name = list(uploaded_image.keys())[0]\n",
        "\n",
        "# Extract text from the uploaded image\n",
        "extracted_text = extract_text_from_image(image_name)\n",
        "\n",
        "# Print the extracted text\n",
        "print(\"Extracted Text:\")\n",
        "print(extracted_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cE6Hf6xasdi"
      },
      "source": [
        "## **This code will then try to detect the gramartical error in the code and it will correct the grammer.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXSO80BgcOse",
        "outputId": "a7127835-affb-434a-9216-09cad87c9c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "I going t homeeee\n",
            "\n",
            "Corrected Text:\n",
            "I'm going t homered\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def correct_text_with_languagetool(text):\n",
        "\n",
        "    # LanguageTool API endpoint\n",
        "    url = \"https://api.languagetoolplus.com/v2/check\"\n",
        "\n",
        "    # API payload\n",
        "    payload = {\n",
        "        \"text\": text,\n",
        "        \"language\": \"en-US\"\n",
        "    }\n",
        "\n",
        "    # Make the API request\n",
        "    response = requests.post(url, data=payload)\n",
        "    result = response.json()\n",
        "\n",
        "    # Apply the corrections to the original text\n",
        "    corrected_text = text\n",
        "    if \"matches\" in result:\n",
        "        for match in reversed(result[\"matches\"]):  # Process matches in reverse order\n",
        "            start = match[\"offset\"]\n",
        "            end = start + match[\"length\"]\n",
        "            replacement = match[\"replacements\"][0][\"value\"] if match[\"replacements\"] else \"\"\n",
        "            corrected_text = corrected_text[:start] + replacement + corrected_text[end:]\n",
        "\n",
        "    return corrected_text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"I going t homeeee\"\n",
        "corrected_text = correct_text_with_languagetool(input_text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(input_text)\n",
        "print(\"\\nCorrected Text:\")\n",
        "print(corrected_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUdp4a7uY7MN"
      },
      "source": [
        "## Improve the accuracy for handwritting text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH_WgV54Y_g7",
        "outputId": "46d08f7a-fa9c-4084-e675-51e683190453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-documentai in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-documentai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-documentai) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-documentai) (4.25.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (1.66.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-documentai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai) (4.9)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-documentai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-documentai spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHgXotNJpgUT",
        "outputId": "30e95a9e-8daf-4819-bdee-ecf83deb0707"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Extracted and Refined Text:\n",
            "I want to be\n",
            "\n",
            "they\n",
            "\n",
            "man. untill they\n",
            "\n",
            "good\n",
            "\n",
            "ŸÑ€å\n",
            "\n",
            "nullify\n",
            "\n",
            "my strength. me\n",
            "\n",
            "and\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import io\n",
        "import os\n",
        "from google.cloud import documentai_v1 as documentai\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "\n",
        "# ‚úÖ Step 1: Extract Text and Bounding Boxes with Document AI\n",
        "def extract_text_with_boxes(project_id, location, processor_id, file_path, json_key_path):\n",
        "\n",
        "    # Create client using the JSON key file\n",
        "    client = documentai.DocumentProcessorServiceClient.from_service_account_json(json_key_path)\n",
        "\n",
        "    # Construct the resource name of the processor\n",
        "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
        "\n",
        "    # Read the file\n",
        "    with io.open(file_path, \"rb\") as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    # Create a raw document object\n",
        "    raw_document = documentai.RawDocument(content=content, mime_type=\"image/jpeg\")\n",
        "\n",
        "    # Configure the request\n",
        "    request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
        "\n",
        "    # Process the document\n",
        "    result = client.process_document(request=request)\n",
        "\n",
        "    # Extract text and bounding box information\n",
        "    document = result.document\n",
        "    text_content = document.text\n",
        "    elements = []\n",
        "\n",
        "    for page in document.pages:\n",
        "        for paragraph in page.paragraphs:\n",
        "            if paragraph.layout.bounding_poly:\n",
        "                bbox = paragraph.layout.bounding_poly.vertices\n",
        "                vertices = [(vertex.x, vertex.y) for vertex in bbox]\n",
        "                text = paragraph.layout.text_anchor.text_segments\n",
        "                paragraph_text = \"\".join(\n",
        "                    [text_content[seg.start_index:seg.end_index] for seg in text]\n",
        "                )\n",
        "                elements.append({\"text\": paragraph_text, \"bbox\": vertices})\n",
        "\n",
        "    return elements\n",
        "\n",
        "\n",
        "# ‚úÖ Step 2: Reorder Text Using Bounding Boxes\n",
        "def reorder_text(elements):\n",
        "    # Sort elements by Y-coordinate (top to bottom) and X-coordinate (left to right)\n",
        "    elements.sort(key=lambda e: (min([v[1] for v in e[\"bbox\"]]), min([v[0] for v in e[\"bbox\"]])))\n",
        "\n",
        "    # Combine text in reading order\n",
        "    reordered_text = \"\\n\".join([e[\"text\"] for e in elements])\n",
        "    return reordered_text\n",
        "\n",
        "\n",
        "# ‚úÖ Step 3: Use NLP for Refinement\n",
        "def refine_text_with_nlp(text):\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_sm\")  # Load English NLP model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Combine sentences into refined paragraphs\n",
        "    refined_text = \" \".join([sent.text.strip() for sent in doc.sents])\n",
        "    return refined_text\n",
        "\n",
        "\n",
        "# ‚úÖ Step 4: Complete Pipeline\n",
        "def process_image_with_context(project_id, location, processor_id, file_path, json_key_path):\n",
        "\n",
        "    # Extract text and bounding boxes\n",
        "    elements = extract_text_with_boxes(project_id, location, processor_id, file_path, json_key_path)\n",
        "\n",
        "    # Reorder text based on bounding boxes\n",
        "    reordered_text = reorder_text(elements)\n",
        "\n",
        "    # Refine text using NLP\n",
        "    final_text = refine_text_with_nlp(reordered_text)\n",
        "    return final_text\n",
        "\n",
        "\n",
        "# ‚úÖ Step 5: Configure API Credentials and Run Pipeline\n",
        "PROJECT_ID = \"corded-skill-446022-r2\"\n",
        "LOCATION = \"us\"  # Use 'us' or 'eu' based on your processor's location\n",
        "PROCESSOR_ID = \"4d451d213280960\"  # Replace with your processor ID\n",
        "IMAGE_PATH = \"/content/drive/MyDrive/Pic_for_Research/myHandWriting2.JPG\"  # Replace with your image path\n",
        "JSON_KEY_PATH = \"/content/drive/MyDrive/google_document_ai/documentAi.json\"  # Replace with your JSON key path\n",
        "\n",
        "# Process the image and extract refined text\n",
        "final_text = process_image_with_context(PROJECT_ID, LOCATION, PROCESSOR_ID, IMAGE_PATH, JSON_KEY_PATH)\n",
        "\n",
        "# Print the final refined text\n",
        "print(\"\\nFinal Extracted and Refined Text:\")\n",
        "print(final_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HIZBj3vQFAg"
      },
      "source": [
        "## Code with NLP + OCR + Bounding box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIrbhVMnQNql",
        "outputId": "afee68ae-bed6-40f8-826e-d4375ffc359e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Extracting text with bounding boxes...\n",
            "üîÑ Reordering text based on bounding boxes...\n",
            "üìù Refining text using NLP for better readability...\n",
            "\n",
            "‚ú® Final Corrected Text:\n",
            "I want to be\n",
            " man. untill they\n",
            " they\n",
            " good\n",
            " ŸÑ€å\n",
            " nullify\n",
            " my strength. me\n",
            " and\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import os\n",
        "import spacy\n",
        "from google.cloud import documentai_v1 as documentai\n",
        "\n",
        "# ‚úÖ Step 1: Extract Text with Bounding Box Positions\n",
        "def extract_text_with_bounding_boxes(project_id, location, processor_id, file_path, json_key_path):\n",
        "    \"\"\"\n",
        "    Extracts text along with bounding box positions from an image using Google Document AI.\n",
        "\n",
        "    Args:\n",
        "        project_id (str): Google Cloud project ID.\n",
        "        location (str): Location of the processor ('us' or 'eu').\n",
        "        processor_id (str): ID of the Document AI processor.\n",
        "        file_path (str): Path to the image.\n",
        "        json_key_path (str): Path to the service account JSON key file.\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted words with bounding box positions.\n",
        "    \"\"\"\n",
        "    client = documentai.DocumentProcessorServiceClient.from_service_account_json(json_key_path)\n",
        "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
        "\n",
        "    with io.open(file_path, \"rb\") as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    raw_document = documentai.RawDocument(content=content, mime_type=\"image/jpeg\")\n",
        "    request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
        "    result = client.process_document(request=request)\n",
        "\n",
        "    document = result.document\n",
        "    extracted_data = []\n",
        "\n",
        "    for page in document.pages:\n",
        "        for paragraph in page.paragraphs:\n",
        "            if paragraph.layout.bounding_poly:\n",
        "                bbox = paragraph.layout.bounding_poly.vertices\n",
        "                vertices = [(vertex.x, vertex.y) for vertex in bbox]\n",
        "                text = paragraph.layout.text_anchor.text_segments\n",
        "                paragraph_text = \"\".join(\n",
        "                    [document.text[seg.start_index:seg.end_index] for seg in text]\n",
        "                )\n",
        "                extracted_data.append({\"text\": paragraph_text, \"bbox\": vertices})\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "\n",
        "# ‚úÖ Step 2: Reorder Text Based on Bounding Box Positions\n",
        "def reorder_text(extracted_data):\n",
        "    \"\"\"\n",
        "    Reorders words based on bounding box positions for correct sentence flow.\n",
        "\n",
        "    Args:\n",
        "        extracted_data (list): List of extracted words with bounding box positions.\n",
        "\n",
        "    Returns:\n",
        "        str: Properly ordered text.\n",
        "    \"\"\"\n",
        "    # Sort words by Y-coordinate (to ensure correct line ordering), then by X (left to right)\n",
        "    extracted_data.sort(key=lambda e: (min(v[1] for v in e[\"bbox\"]), min(v[0] for v in e[\"bbox\"])))\n",
        "\n",
        "    # Combine words into sentences\n",
        "    ordered_text = \" \".join([e[\"text\"] for e in extracted_data])\n",
        "    return ordered_text\n",
        "\n",
        "\n",
        "# ‚úÖ Step 3: Fix Grammar & Sentence Structure Using NLP\n",
        "def refine_text_with_nlp(text):\n",
        "    \"\"\"\n",
        "    Refines the extracted text using NLP to improve sentence flow.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw extracted text.\n",
        "\n",
        "    Returns:\n",
        "        str: Refined text with corrected grammar.\n",
        "    \"\"\"\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "\n",
        "    refined_text = \" \".join([sent.text.strip() for sent in doc.sents])\n",
        "    return refined_text\n",
        "\n",
        "\n",
        "# ‚úÖ Step 4: Full Pipeline - Correct Text Order from OCR\n",
        "def process_image_for_correct_text_order(project_id, location, processor_id, file_path, json_key_path):\n",
        "    \"\"\"\n",
        "    Extracts text from an image, reorders it using bounding box positions, and refines grammar using NLP.\n",
        "\n",
        "    Args:\n",
        "        project_id (str): Google Cloud project ID.\n",
        "        location (str): Location of the processor ('us' or 'eu').\n",
        "        processor_id (str): ID of the Document AI processor.\n",
        "        file_path (str): Path to the image.\n",
        "        json_key_path (str): Path to the service account JSON key file.\n",
        "\n",
        "    Returns:\n",
        "        str: Final corrected text.\n",
        "    \"\"\"\n",
        "    print(\"üîç Extracting text with bounding boxes...\")\n",
        "    extracted_data = extract_text_with_bounding_boxes(project_id, location, processor_id, file_path, json_key_path)\n",
        "\n",
        "    print(\"üîÑ Reordering text based on bounding boxes...\")\n",
        "    ordered_text = reorder_text(extracted_data)\n",
        "\n",
        "    print(\"üìù Refining text using NLP for better readability...\")\n",
        "    final_text = refine_text_with_nlp(ordered_text)\n",
        "\n",
        "    return final_text\n",
        "\n",
        "\n",
        "# ‚úÖ Step 5: Configure API Credentials and Run Pipeline\n",
        "PROJECT_ID = \"corded-skill-446022-r2\"\n",
        "LOCATION = \"us\"  # Use 'us' or 'eu' based on your processor's location\n",
        "PROCESSOR_ID = \"4d451d213280960\"  # Replace with your processor ID\n",
        "IMAGE_PATH = \"/content/drive/MyDrive/Pic_for_Research/myHandWriting2.JPG\"  # Replace with your image path\n",
        "JSON_KEY_PATH = \"/content/drive/MyDrive/google_document_ai/documentAi.json\"  # Replace with your JSON key path\n",
        "\n",
        "# ‚úÖ Run the OCR Correction Pipeline\n",
        "corrected_text = process_image_for_correct_text_order(PROJECT_ID, LOCATION, PROCESSOR_ID, IMAGE_PATH, JSON_KEY_PATH)\n",
        "\n",
        "# ‚úÖ Step 6: Print the Final Corrected Text\n",
        "print(\"\\n‚ú® Final Corrected Text:\")\n",
        "print(corrected_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75P93Z4qRmLJ"
      },
      "source": [
        "## Working with NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F0WXlngYQuY",
        "outputId": "46d77567-9bfe-48e4-d4b0-77f10aacf5c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 90074.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ 500-Sentence Dataset Created & Saved!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ‚úÖ Base correct sentences (expand this list for more variety)\n",
        "correct_sentences = [\n",
        "    \"I want to be a good person.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"He is studying machine learning.\",\n",
        "    \"She loves reading books in the evening.\",\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"Python is my favorite programming language.\",\n",
        "    \"We need to prepare for the final exam.\",\n",
        "    \"The cat is sitting on the windowsill.\",\n",
        "    \"Tomorrow we will visit the museum.\",\n",
        "    \"Learning new skills is important for personal growth.\",\n",
        "    \"He enjoys watching football on weekends.\",\n",
        "    \"The sun is shining brightly in the sky.\",\n",
        "    \"They are planning a trip to the mountains.\",\n",
        "    \"I have an important meeting at 10 AM.\",\n",
        "    \"Exercising regularly keeps you healthy.\",\n",
        "    \"She bought a beautiful dress for the party.\",\n",
        "    \"We should always be kind to others.\",\n",
        "    \"The coffee shop is located near the library.\",\n",
        "    \"He is writing an article about climate change.\",\n",
        "    \"Traveling helps people understand different cultures.\",\n",
        "    \"Mathematics is essential for engineering and science.\",\n",
        "    \"She practices yoga every morning to stay fit.\",\n",
        "    \"They adopted a puppy from the animal shelter.\",\n",
        "    \"I need to finish my assignment before the deadline.\",\n",
        "    \"History teaches us valuable lessons about the past.\",\n",
        "    \"Cooking at home is healthier than eating out.\",\n",
        "    \"Technology is advancing at a rapid pace.\",\n",
        "    \"He is trying to learn a new language.\",\n",
        "    \"The artist is painting a beautiful landscape.\",\n",
        "    \"Reading helps improve vocabulary and comprehension.\",\n",
        "    \"I love to read books on weekends.\",\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"He is learning how to code in Python.\",\n",
        "    \"We are planning a trip to the mountains.\",\n",
        "    \"She enjoys baking cakes in her free time.\",\n",
        "    \"The sun is shining brightly in the sky.\",\n",
        "    \"I want to be a successful person.\",\n",
        "    \"They are watching a movie together.\",\n",
        "    \"Learning new skills is important for personal growth.\",\n",
        "    \"He enjoys playing football with his friends.\",\n",
        "    \"She writes poems about nature and life.\",\n",
        "    \"The cat is sitting on the windowsill.\",\n",
        "    \"We should always be kind to others.\",\n",
        "    \"He is preparing for his final exams.\",\n",
        "    \"The train arrives at the station in the evening.\",\n",
        "    \"She practices yoga every morning to stay fit.\",\n",
        "    \"They adopted a puppy from the animal shelter.\",\n",
        "    \"History teaches us valuable lessons about the past.\",\n",
        "    \"Cooking at home is healthier than eating out.\",\n",
        "    \"Technology is advancing at a rapid pace.\",\n",
        "    \"The artist is painting a beautiful landscape.\",\n",
        "    \"The students are studying for their math test.\",\n",
        "    \"The coffee shop is located near the library.\",\n",
        "    \"Traveling helps people understand different cultures.\",\n",
        "    \"Exercising regularly keeps you healthy and active.\",\n",
        "]\n",
        "\n",
        "# ‚úÖ Generate 500 shuffled sentence pairs\n",
        "dataset = []\n",
        "for _ in tqdm(range(5000)):  # Create 500 unique data points\n",
        "    sentence = random.choice(correct_sentences)  # Pick a random sentence\n",
        "    words = sentence.split()\n",
        "    shuffled_words = words.copy()\n",
        "    random.shuffle(shuffled_words)  # Shuffle words randomly\n",
        "    shuffled_sentence = \" \".join(shuffled_words)\n",
        "\n",
        "    dataset.append({\"input\": f\"reorder: {shuffled_sentence}\", \"output\": sentence})\n",
        "\n",
        "# ‚úÖ Convert dataset to DataFrame\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# ‚úÖ Save dataset as CSV for future use\n",
        "df.to_csv(\"sentence_reordering_dataset.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ 500-Sentence Dataset Created & Saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhFuD4xrYWXn",
        "outputId": "9abdcfd6-ad3a-441e-f42c-dec8bed17097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    input  \\\n",
            "179061     reorder: is cat sitting the windowsill. The on   \n",
            "289002   reorder: The brightly the sun in shining is sky.   \n",
            "82403   reorder: at is healthier home out. eating than...   \n",
            "191726  reorder: from puppy animal shelter. They a the...   \n",
            "118234  reorder: advancing Technology at is a rapid pace.   \n",
            "\n",
            "                                               output  \n",
            "179061          The cat is sitting on the windowsill.  \n",
            "289002        The sun is shining brightly in the sky.  \n",
            "82403   Cooking at home is healthier than eating out.  \n",
            "191726  They adopted a puppy from the animal shelter.  \n",
            "118234       Technology is advancing at a rapid pace.  \n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Load and print 5 random examples\n",
        "df = pd.read_csv(\"sentence_reordering_dataset.csv\")\n",
        "print(df.sample(5))  # Show 5 random sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l4uy1kqYd4P",
        "outputId": "0e29e190-dadd-4e16-f4c8-0e87499e2cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Training Model on Sentence Reordering...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [1:22:36<00:00,  1.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 236.7699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [1:24:13<00:00,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 0.4762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 4194/5000 [1:12:54<13:16,  1.01it/s]"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ‚úÖ Load dataset\n",
        "df = pd.read_csv(\"sentence_reordering_dataset.csv\")\n",
        "train_data = df.to_dict(\"records\")\n",
        "\n",
        "# ‚úÖ Load pre-trained T5 model & tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# ‚úÖ Tokenize input and output\n",
        "input_texts = [tokenizer(item[\"input\"], return_tensors=\"pt\", padding=True, truncation=True) for item in train_data]\n",
        "output_texts = [tokenizer(item[\"output\"], return_tensors=\"pt\", padding=True, truncation=True) for item in train_data]\n",
        "\n",
        "# ‚úÖ Set up optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# ‚úÖ Train for 10 epochs on 500 sentences\n",
        "print(\"üîÑ Training Model on Sentence Reordering...\")\n",
        "for epoch in range(25):\n",
        "    total_loss = 0\n",
        "    for inp, out in tqdm(zip(input_texts, output_texts), total=len(input_texts)):\n",
        "        outputs = model(**inp, labels=out[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# ‚úÖ Save fine-tuned model\n",
        "model.save_pretrained(\"t5_sentence_reorder\")\n",
        "tokenizer.save_pretrained(\"t5_sentence_reorder\")\n",
        "print(\"‚úÖ Model Training Complete & Saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "CSyXtY7bNGuZ",
        "outputId": "9a8b8581-04c7-4d4d-d863-4f7d832b9bb4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'T5ForConditionalGeneration' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bfaa678216e7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ‚úÖ Load Fine-Tuned Model for Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t5_sentence_reorder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t5_sentence_reorder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorrect_sentence_t5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'T5ForConditionalGeneration' is not defined"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Load Fine-Tuned Model for Inference\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5_sentence_reorder\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5_sentence_reorder\")\n",
        "\n",
        "def correct_sentence_t5(input_text):\n",
        "    \"\"\"\n",
        "    Uses fine-tuned T5 model to correct OCR sentence order.\n",
        "    \"\"\"\n",
        "    input_text = \"reorder: \" + input_text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(input_ids, max_length=50)\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ‚úÖ Test the model with NEW, unseen sentences\n",
        "test_sentences = [\n",
        "    \"i be want man a good\",\n",
        "    \"mother is name my Shilpi\",\n",
        "    \"much love i her love so\",\n",
        "    \"how he to it do knows\"\n",
        "]\n",
        "\n",
        "print(\"\\nüîπ Example Predictions:\")\n",
        "for sentence in test_sentences:\n",
        "    print(f\"üìù Input: {sentence}\")\n",
        "    print(f\"‚úÖ Output: {correct_sentence_t5(sentence)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with LSTM Based pointer network"
      ],
      "metadata": {
        "id": "dDoaGZVHvjSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy pandas tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6a6fIdbvs_c",
        "outputId": "c6932b3c-fa6e-4cf6-8226-f7e2200b2a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# ‚úÖ Define Correctly Ordered Sentences\n",
        "correct_sentences = [\n",
        "    \"I want to be a good man\",\n",
        "    \"My mother's name is Shilpi\",\n",
        "    \"I love her so much\",\n",
        "    \"He knows how to do it\",\n",
        "    \"The sun is shining brightly today\",\n",
        "    \"She enjoys baking cakes in her free time\",\n",
        "    \"Technology is advancing at a rapid pace\",\n",
        "]\n",
        "\n",
        "# ‚úÖ Generate Training Data (Shuffled Sentences)\n",
        "def generate_training_data(sentences, num_samples=500):\n",
        "    training_data = []\n",
        "    for _ in range(num_samples):\n",
        "        sentence = random.choice(sentences)  # Pick a correct sentence\n",
        "        words = sentence.split()\n",
        "        shuffled_words = words.copy()\n",
        "        random.shuffle(shuffled_words)  # Shuffle words randomly\n",
        "        training_data.append({\"input\": shuffled_words, \"output\": words})\n",
        "    return training_data\n",
        "\n",
        "# ‚úÖ Create dataset\n",
        "dataset = generate_training_data(correct_sentences, num_samples=500)\n",
        "\n",
        "# ‚úÖ Convert to DataFrame and Save\n",
        "df = pd.DataFrame(dataset)\n",
        "df.to_csv(\"sentence_reordering_dataset.csv\", index=False)\n",
        "print(\"‚úÖ Dataset Created and Saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMjOaqyS3sLV",
        "outputId": "2f798ffd-0c17-4ec5-9e4b-ea6e65695aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset Created and Saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Load Dataset\n",
        "df = pd.read_csv(\"sentence_reordering_dataset.csv\")\n",
        "\n",
        "# ‚úÖ Build Vocabulary\n",
        "unique_words = set(word for sentence in df[\"input\"] for word in sentence.split())\n",
        "word2idx = {word: idx + 1 for idx, word in enumerate(unique_words)}  # +1 for padding\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# ‚úÖ Add <UNK> Token for Unseen Words\n",
        "word2idx[\"<UNK>\"] = len(word2idx) + 1\n",
        "idx2word[len(word2idx)] = \"<UNK>\"\n",
        "\n",
        "print(\"‚úÖ Vocabulary Size:\", len(word2idx))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5gyO_Yc3wV2",
        "outputId": "2d49edda-fe67-4cda-dac7-d7a0aba56591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Vocabulary Size: 115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Function to encode sentences safely\n",
        "def encode_sentence(sentence, word2idx):\n",
        "    return [word2idx.get(word, word2idx[\"<UNK>\"]) for word in sentence.split()]\n",
        "\n",
        "# ‚úÖ Encode input and output sentences\n",
        "df[\"input_encoded\"] = df[\"input\"].apply(lambda x: encode_sentence(x, word2idx))\n",
        "df[\"output_encoded\"] = df[\"output\"].apply(lambda x: encode_sentence(x, word2idx))\n",
        "\n",
        "# ‚úÖ Get max sequence length\n",
        "max_length = max(df[\"input_encoded\"].apply(len))\n",
        "\n",
        "# ‚úÖ Function to pad sequences\n",
        "def pad_sequence(seq, max_length):\n",
        "    return seq + [0] * (max_length - len(seq))  # Pad with 0s\n",
        "\n",
        "# ‚úÖ Apply padding\n",
        "df[\"input_encoded\"] = df[\"input_encoded\"].apply(lambda x: pad_sequence(x, max_length))\n",
        "df[\"output_encoded\"] = df[\"output_encoded\"].apply(lambda x: pad_sequence(x, max_length))\n",
        "\n",
        "# ‚úÖ Convert to PyTorch tensors\n",
        "X_train = torch.tensor(df[\"input_encoded\"].tolist(), dtype=torch.long)\n",
        "y_train = torch.tensor(df[\"output_encoded\"].tolist(), dtype=torch.long)\n",
        "\n",
        "print(\"‚úÖ Data Processed Successfully! Training Samples:\", len(X_train))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9GRwqUn31Qw",
        "outputId": "f58a28c7-6e14-4ff4-cb4e-cb207b66d177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data Processed Successfully! Training Samples: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PointerLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(PointerLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Linear(hidden_dim * 2, max_length)  # Attention layer\n",
        "        self.output_layer = nn.Linear(hidden_dim * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # Convert words to embeddings\n",
        "        lstm_out, _ = self.lstm(embedded)  # Pass through LSTM\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "\n",
        "        # Compute output using weighted sum\n",
        "        output = torch.matmul(attention_scores.transpose(1, 2), lstm_out)\n",
        "        output = self.output_layer(output)\n",
        "\n",
        "        return output, attention_scores\n"
      ],
      "metadata": {
        "id": "qGKDTCCP34sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Model Parameters\n",
        "embedding_dim = 64\n",
        "hidden_dim = 128\n",
        "vocab_size = len(word2idx) + 1  # +1 for padding\n",
        "model = PointerLSTM(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# ‚úÖ Training Parameters\n",
        "epochs = 25\n",
        "learning_rate = 0.005\n",
        "\n",
        "# ‚úÖ Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ‚úÖ Training Loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i in range(len(X_train)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        input_seq = X_train[i].unsqueeze(0)\n",
        "        target_seq = y_train[i].unsqueeze(0)\n",
        "        output, _ = model(input_seq)\n",
        "\n",
        "        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"‚úÖ Epoch {epoch+1}, Loss: {total_loss / len(X_train):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w863yGrc37r3",
        "outputId": "1c71ccc7-87f9-41e5-c2c8-e01812320473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1, Loss: 0.8349\n",
            "‚úÖ Epoch 2, Loss: 0.1017\n",
            "‚úÖ Epoch 3, Loss: 0.0427\n",
            "‚úÖ Epoch 4, Loss: 0.0491\n",
            "‚úÖ Epoch 5, Loss: 0.0628\n",
            "‚úÖ Epoch 6, Loss: 0.0826\n",
            "‚úÖ Epoch 7, Loss: 0.0612\n",
            "‚úÖ Epoch 8, Loss: 0.0325\n",
            "‚úÖ Epoch 9, Loss: 0.0324\n",
            "‚úÖ Epoch 10, Loss: 0.0335\n",
            "‚úÖ Epoch 11, Loss: 0.0275\n",
            "‚úÖ Epoch 12, Loss: 0.0308\n",
            "‚úÖ Epoch 13, Loss: 0.1062\n",
            "‚úÖ Epoch 14, Loss: 0.0726\n",
            "‚úÖ Epoch 15, Loss: 0.0500\n",
            "‚úÖ Epoch 16, Loss: 0.0303\n",
            "‚úÖ Epoch 17, Loss: 0.0293\n",
            "‚úÖ Epoch 18, Loss: 0.0107\n",
            "‚úÖ Epoch 19, Loss: 0.0163\n",
            "‚úÖ Epoch 20, Loss: 0.0506\n",
            "‚úÖ Epoch 21, Loss: 0.0441\n",
            "‚úÖ Epoch 22, Loss: 0.0254\n",
            "‚úÖ Epoch 23, Loss: 0.0177\n",
            "‚úÖ Epoch 24, Loss: 0.0206\n",
            "‚úÖ Epoch 25, Loss: 0.0053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/MdSourav76046/My_Thesis.git\n",
        "!git branch -M main\n",
        "!git push -u origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3dZ6ltINJmQ",
        "outputId": "810e608b-96ba-4f72-a743-7a17e94de37d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "w_Xqc9YhYLK6",
        "5po-wqOHaIuZ"
      ],
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "16-FbtZBtcIeWpqeeJ9McdyCO-xK8-R7R",
      "authorship_tag": "ABX9TyO70ez0OcSpBw/H6FLDdVt1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}